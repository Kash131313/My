#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import time
import sqlite3
import asyncio
import logging
import concurrent.futures
import tempfile
import shutil
from functools import partial
from datetime import datetime
from typing import List, Tuple, Optional

import pytz
from dateutil import tz
from telethon import TelegramClient, events, errors
from openpyxl import Workbook, load_workbook
from urllib.parse import urlsplit, urlunsplit

# ----------------------------- CONFIG -----------------------------
API_ID = int(os.environ.get('TG_API_ID', '34599696'))
API_HASH = os.environ.get('TG_API_HASH', '1499462b8ad56f15bf407582b7a2175a')
PHONE = os.environ.get('TG_PHONE', '+79256991300')
TWOFA_PASSWORD = os.environ.get('TG_2FA_PASSWORD', '')  # optional
TARGET_CHANNEL = os.environ.get('TG_TARGET_CHANNEL', '-1003432562272')

EXCEL_PATH = "C:/Users/–ò–≤–∞–Ω/Desktop/links.xlsx"
STATE_DB = 'state_links.db'
SESSION_NAME = 'telegram_link_parser.session'

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
SOURCES = [
    -1003497455949,
]

# –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã/–Ω–∞—Å—Ç—Ä–æ–π–∫–∏
MAX_MESSAGES_POLL = 200
TIMEZONE = 'Europe/Moscow'
CHANNEL_FETCH_LIMIT = 5000  # —Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤—ã—Ç—è–Ω—É—Ç—å –∏–∑ —Ü–µ–ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
DB_BUSY_RETRIES = 8

# –†–µ–∞–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º
REACTIONS = ['üëç', 'üî•', 'ü§ù']

# –û—á–∏—Å—Ç–∫–∞ –∫–∞–Ω–∞–ª–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ True ‚Äî –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è –æ—á–∏—Å—Ç–∏—Ç—å –∫–∞–Ω–∞–ª –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ; –µ—Å–ª–∏ False, –º—ã –ø–æ–ø—ã—Ç–∞–µ–º—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å)
CLEAR_TARGET_CHANNEL_ON_START = True

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger('linkparser')

# –†–µ–≥–µ–∫—Å –¥–ª—è URL - –±–∞–∑–æ–≤—ã–π
URL_RE = re.compile(r"(https?://\S+)")
HYPERLINK_RE = re.compile(r'HYPERLINK\("([^"]+)"\s*,\s*"([^"]*)"\)', re.IGNORECASE)

# -------------------------- DB WORKER ------------------------------
DB_PATH = STATE_DB
_db_queue: asyncio.Queue = asyncio.Queue()
_executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
_db_conn_for_worker: Optional[sqlite3.Connection] = None

def _open_sqlite_conn_for_worker(path: str) -> sqlite3.Connection:
    conn = sqlite3.connect(path, timeout=30, check_same_thread=False)
    try:
        conn.execute('PRAGMA journal_mode=WAL;')
    except Exception:
        pass
    try:
        conn.execute('PRAGMA synchronous=NORMAL;')
    except Exception:
        pass
    try:
        conn.execute('PRAGMA busy_timeout=5000;')
    except Exception:
        pass
    return conn

def _sqlite_exec(conn: sqlite3.Connection, sql: str, params: Tuple = ()):
    cur = conn.cursor()
    cur.execute(sql, params or ())
    if sql.strip().upper().startswith('INSERT'):
        conn.commit()
        return cur.lastrowid
    else:
        rows = cur.fetchall()
        conn.commit()
        return rows

async def db_worker():
    global _db_conn_for_worker
    loop = asyncio.get_running_loop()
    _db_conn_for_worker = _open_sqlite_conn_for_worker(DB_PATH)
    logger.info("DB worker started")
    while True:
        item = await _db_queue.get()
        if item is None:
            _db_queue.task_done()
            break
        sql, params, fut = item
        try:
            func = partial(_sqlite_exec, _db_conn_for_worker, sql, params)
            res = await loop.run_in_executor(_executor, func)
            if fut and not fut.cancelled():
                fut.set_result(res)
        except Exception as e:
            logger.exception("DB worker error executing SQL")
            if fut and not fut.cancelled():
                fut.set_exception(e)
        finally:
            _db_queue.task_done()
    try:
        _db_conn_for_worker.close()
    except Exception:
        pass
    logger.info("DB worker stopped")

async def enqueue_db(sql: str, params: Tuple = (), wait_result: bool = True):
    if wait_result:
        fut = asyncio.get_running_loop().create_future()
    else:
        fut = None
    await _db_queue.put((sql, params, fut))
    if fut:
        return await fut
    return None

async def db_fetchall(sql: str, params: Tuple = ()):
    res = await enqueue_db(sql, params, wait_result=True)
    return res or []

async def db_execute(sql: str, params: Tuple = ()):
    return await enqueue_db(sql, params, wait_result=True)

# ------------------------- EXCEL HELPERS -------------------------

def ensure_excel(path: str):
    if not os.path.exists(path):
        wb = Workbook()
        ws = wb.active
        ws.title = 'links'
        ws['A1'] = 'link'
        ws['B1'] = 'time'
        wb.save(path)
        logger.info(f'Created new Excel file at {path}')

def format_time_for_excel(ts: int) -> str:
    dt = datetime.fromtimestamp(ts, tz=pytz.utc).astimezone(tz.gettz(TIMEZONE))
    return dt.strftime('%H:%M-%d.%m.%y')

def save_workbook_atomic(wb, target_path):
    dir_name = os.path.dirname(target_path) or "."
    fd, tmp_path = tempfile.mkstemp(dir=dir_name, prefix=".tmp_excel_", suffix=".xlsx")
    os.close(fd)
    try:
        wb.save(tmp_path)
        shutil.move(tmp_path, target_path)
    finally:
        if os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except Exception:
                pass

async def rebuild_excel_from_db(path: str):
    loop = asyncio.get_running_loop()
    now = int(time.time())
    cutoff = now - 24 * 3600
    rows = await db_fetchall('SELECT url, first_seen FROM urls WHERE first_seen >= ? ORDER BY first_seen ASC', (cutoff,))
    try:
        def _write(rows_local):
            ensure_excel(path)
            wb = load_workbook(path)
            ws = wb.active
            if ws.max_row >= 2:
                ws.delete_rows(2, ws.max_row)
            r = 2
            for url, first_seen in rows_local:
                ws.cell(row=r, column=1).value = f'=HYPERLINK("{url}","{url}")'
                ws.cell(row=r, column=2).value = format_time_for_excel(first_seen)
                r += 1
            save_workbook_atomic(wb, path)
            return len(rows_local)
        cnt = await loop.run_in_executor(_executor, partial(_write, rows))
        logger.info(f'Excel rebuilt with {cnt} rows')
    except Exception as e:
        logger.exception(f'Cannot rebuild Excel: {e}')

# ------------------------- URL / NORM HELPERS ---------------------

def normalize_url(u: str) -> str:
    if not u:
        return u
    u = u.strip().rstrip('.,;:!?\'"')
    has_scheme = '://' in u
    parse_target = u if has_scheme else ('http://' + u)
    try:
        parts = urlsplit(parse_target)
    except Exception:
        return u
    scheme = parts.scheme.lower() if parts.scheme else 'http'
    netloc = parts.netloc.lower()
    if netloc.startswith('www.'):
        netloc = netloc[4:]
    if ':' in netloc:
        host, port = netloc.split(':', 1)
        if port in ('80','443'):
            netloc = host
    path = parts.path or '/'
    query = parts.query or ''
    frag = ''
    return urlunsplit((scheme, netloc, path, query, frag))

# -------------------------- TELETHON APP -------------------------

client = TelegramClient(SESSION_NAME, API_ID, API_HASH)

async def send_channel_message(url: str, msg_ts: int) -> Optional[int]:
    try:
        try:
            target = int(TARGET_CHANNEL)
        except Exception:
            target = TARGET_CHANNEL
        text = f'{url}\n{format_time_for_excel(msg_ts)}'
        msg = await client.send_message(entity=target, message=text)
        logger.info(f'Sent to channel {target} msg_id={msg.id} url={url}')
        return msg.id
    except Exception as e:
        logger.exception(f'Failed to send to channel: {e}')
        return None

async def delete_channel_message(msg_id: int):
    if not msg_id:
        return
    try:
        try:
            target = int(TARGET_CHANNEL)
        except Exception:
            target = TARGET_CHANNEL
        await client.delete_messages(entity=target, message_ids=[msg_id])
        logger.info(f'Deleted message in channel id={msg_id}')
    except Exception as e:
        logger.warning(f'Failed to delete channel message {msg_id}: {e}')

async def try_send_reaction(peer, msg_id, reaction: str) -> bool:
    try:
        if hasattr(client, 'send_reaction'):
            await client.send_reaction(entity=peer, message=msg_id, reaction=reaction)
            return True
        return False
    except errors.RPCError as e:
        logger.warning(f'RPC error sending reaction {reaction} to {peer}:{msg_id} -> {e}')
        return False
    except Exception as e:
        logger.warning(f'Cannot send reaction {reaction} to {peer}:{msg_id} -> {e}')
        return False

# ---------------------- Reaction sync / helpers --------------------

async def gather_channel_messages_with_urls(limit=CHANNEL_FETCH_LIMIT):
    try:
        try:
            target = int(TARGET_CHANNEL)
        except Exception:
            target = TARGET_CHANNEL
        msgs = await client.get_messages(entity=target, limit=limit)
    except Exception as e:
        logger.exception(f'Failed to fetch channel messages: {e}')
        return []
    out = []
    for m in msgs:
        if not m or not getattr(m, 'message', None):
            continue
        text = m.message
        found = URL_RE.findall(text)
        if not found:
            continue
        url_text = found[0].strip()
        norm = normalize_url(url_text)
        try:
            ts = int(m.date.replace(tzinfo=pytz.utc).timestamp())
        except Exception:
            ts = 0
        out.append((m.id, text, norm, ts, getattr(m, 'reactions', None)))
    out = list(reversed(out))  # oldest -> newest
    return out

def parse_reaction_flags(reactions_obj) -> List[str]:
    if not reactions_obj:
        return []
    s = str(reactions_obj)
    flags = []
    for r in REACTIONS:
        if r in s:
            flags.append(r)
    return flags

# ------------------------ CHANNEL <-> DB SYNC HELPERS ----------------

async def fetch_db_urls_ordered(cutoff: int) -> List[Tuple[int, str, int, Optional[int]]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (id, url, first_seen, sent_channel_msg_id) —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –ø–æ first_seen asc
    –¢–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ —Å first_seen >= cutoff.
    """
    rows = await db_fetchall('SELECT id, url, first_seen, sent_channel_msg_id FROM urls WHERE first_seen >= ? ORDER BY first_seen ASC', (cutoff,))
    return rows

async def get_channel_url_list(limit=CHANNEL_FETCH_LIMIT) -> List[Tuple[int, str, int]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ (msg.id, normalized_url, ts) –∏–∑ –∫–∞–Ω–∞–ª–∞ –≤ –ø–æ—Ä—è–¥–∫–µ –æ—Ç —Å—Ç–∞—Ä—ã—Ö –∫ –Ω–æ–≤—ã–º.
    """
    rows = await gather_channel_messages_with_urls(limit=limit)
    return [(mid, norm, ts) for (mid, text, norm, ts, reactions) in rows]

async def ensure_channel_matches_db(cutoff: int):
    """
    –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–Ω–∞–ª–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç DB (urls —Å first_seen >= cutoff) –ø–æ –Ω–∞–±–æ—Ä—É –∏ –ø–æ—Ä—è–¥–∫—É.
    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1) –ø–æ–ª—É—á–∞–µ–º desired ordered list (–ø–æ first_seen)
      2) –ø–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∫–∞–Ω–∞–ª–µ (ordered)
      3) –µ—Å–ª–∏ lists —Å–æ–≤–ø–∞–¥–∞—é—Ç –ø–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ urls -> –ø—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ–º sent_channel_msg_id –≤ –ë–î (–µ—Å–ª–∏
         —Å–æ–≤–ø–∞–¥–∞–µ—Ç url -> –∑–∞–ø–∏—Å—ã–≤–∞–µ–º msg.id –≤ sent_channel_msg_id)
      4) –∏–Ω–∞—á–µ -> –æ—á–∏—â–∞–µ–º –∫–∞–Ω–∞–ª –∏ –∑–∞–Ω–æ–≤–æ –∑–∞–ª–∏–≤–∞–µ–º desired –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ (–∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º sent_channel_msg_id)
    """
    desired = await fetch_db_urls_ordered(cutoff)
    desired_urls = [row[1] for row in desired]
    channel_msgs = await get_channel_url_list()
    channel_urls = [norm for (_, norm, _) in channel_msgs]

    if channel_urls == desired_urls:
        logger.info("Channel already matches DB order ‚Äî updating DB mapping of sent_channel_msg_id where needed.")
        # –û–±–Ω–æ–≤–∏–º sent_channel_msg_id –≤ –ë–î, —á—Ç–æ–±—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å id'—ã (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—á–∏—Å—Ç–∏–ª —á–∞—Å—Ç–∏—á–Ω–æ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–ª, –∏–ª–∏
        # –µ—Å–ª–∏ –ë–î –Ω–µ –∑–Ω–∞–ª–∞ —Ç–µ–∫—É—â–∏–π id)
        # –°–æ–∑–¥–∞–¥–∏–º mapping url -> msg.id
        url_to_msgid = {norm: mid for (mid, norm, ts) in channel_msgs}
        for url_id, url, first_seen, sent_msg in desired:
            mid = url_to_msgid.get(url)
            if mid and sent_msg != mid:
                try:
                    await db_execute('UPDATE urls SET sent_channel_msg_id=? WHERE id=?', (mid, url_id))
                except Exception:
                    logger.exception('Failed to update sent_channel_msg_id during sync')
        logger.info("DB mapped to current channel messages.")
        return

    # –ï—Å–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç ‚Äî –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–∞–Ω–∞–ª: –æ—á–∏—â–∞–µ–º –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    logger.info("Channel content differs from DB desired list: rebuilding channel to match DB order.")
    # –æ—á–∏—Å—Ç–∫–∞
    try:
        try:
            target = int(TARGET_CHANNEL)
        except Exception:
            target = TARGET_CHANNEL
        msgs = await client.get_messages(entity=target, limit=CHANNEL_FETCH_LIMIT)
        ids = [m.id for m in msgs if getattr(m, 'id', None) is not None]
        BATCH = 100
        for i in range(0, len(ids), BATCH):
            batch = ids[i:i+BATCH]
            try:
                await client.delete_messages(entity=target, message_ids=batch)
                await asyncio.sleep(0.2)
            except Exception as e:
                logger.warning(f"Failed to delete batch while rebuilding channel: {e}")
        logger.info("Target channel cleared for rebuild.")
    except Exception:
        logger.exception("Failed to clear target channel during rebuild.")

    # –û—Ç–ø—Ä–∞–≤–∏–º –≤ –∫–∞–Ω–∞–ª–µ –≤—Å–µ desired –≤ –ø–æ—Ä—è–¥–∫–µ –∏ –æ–±–Ω–æ–≤–∏–º sent_channel_msg_id
    sent = 0
    for url_id, url, first_seen, sent_msg in desired:
        try:
            new_mid = await send_channel_message(url, first_seen)
            if new_mid:
                try:
                    await db_execute('UPDATE urls SET sent_channel_msg_id=? WHERE id=?', (new_mid, url_id))
                except Exception:
                    logger.exception('Failed to update sent_channel_msg_id after sending during rebuild')
                sent += 1
            await asyncio.sleep(0.2)
        except Exception:
            logger.exception('Failed to send message during channel rebuild')
    logger.info(f"Rebuilt channel with {sent} messages to match DB.")

# ------------------------ STARTUP SEQUENCE ------------------------

async def init_db_tables():
    await db_execute('''
    CREATE TABLE IF NOT EXISTS urls (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url TEXT UNIQUE,
        first_seen INTEGER,
        sent_channel_msg_id INTEGER,
        reacted_flags TEXT DEFAULT ''
    )
    ''')
    await db_execute('''
    CREATE TABLE IF NOT EXISTS occurrences (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url_id INTEGER,
        peer TEXT,
        message_id INTEGER,
        message_date INTEGER,
        FOREIGN KEY(url_id) REFERENCES urls(id)
    )
    ''')

async def apply_reactions_from_channel_to_sources():
    logger.info('Starting reaction sync from channel...')
    rows = await gather_channel_messages_with_urls(limit=CHANNEL_FETCH_LIMIT)
    if not rows:
        logger.info('No messages found in channel for reaction sync.')
        return 0
    applied_total = 0
    for msg_id, text, norm, ts, reactions_obj in rows:
        flags = parse_reaction_flags(reactions_obj)
        if not flags:
            continue
        res = await db_fetchall('SELECT id, reacted_flags FROM urls WHERE url = ?', (norm,))
        if not res:
            continue
        url_id, reacted_flags_db = res[0]
        reacted_flags_db = reacted_flags_db or ''
        occs = await db_fetchall('SELECT peer, message_id FROM occurrences WHERE url_id = ?', (url_id,))
        for peer_str, occ_msg_id in occs:
            try:
                peer = int(peer_str)
            except Exception:
                peer = peer_str
            for r in flags:
                if r in reacted_flags_db:
                    continue
                ok = await try_send_reaction(peer, occ_msg_id, r)
                if ok:
                    applied_total += 1
                    await asyncio.sleep(0.25)
        if flags:
            new_flags = ''.join(sorted(set((reacted_flags_db or '') + ''.join(flags))))
            try:
                await db_execute('UPDATE urls SET reacted_flags=? WHERE id=?', (new_flags, url_id))
            except Exception:
                logger.exception('Failed to update reacted_flags in DB')
    logger.info(f'Reaction sync finished ‚Äî applied approx: {applied_total} reactions')
    return applied_total

async def cleanup_old_links():
    logger.info('Starting cleanup of old links (>24h)...')
    now = int(time.time())
    cutoff = now - 24 * 3600
    rows = await db_fetchall('SELECT id, sent_channel_msg_id FROM urls WHERE first_seen < ?', (cutoff,))
    removed = 0
    for url_id, sent_msg_id in rows:
        if sent_msg_id:
            try:
                await delete_channel_message(sent_msg_id)
            except Exception:
                logger.exception('delete_channel_message failed during cleanup')
        try:
            await db_execute('DELETE FROM occurrences WHERE url_id=?', (url_id,))
            await db_execute('DELETE FROM urls WHERE id=?', (url_id,))
            removed += 1
            logger.info(f'Removed url id={url_id} older than 24h')
        except Exception:
            logger.exception('Failed to delete DB rows during cleanup')
    logger.info(f'Cleanup finished. Removed {removed} old urls.')
    return removed

async def scan_sources_and_insert_recent(cutoff: int):
    """
    –°–∫–∞–Ω–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –≤—Å—Ç–∞–≤–ª—è–µ–º occurrence'—ã –∏ –Ω–æ–≤—ã–µ urls –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π –º–ª–∞–¥—à–µ cutoff.
    –ù–ï –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ –∫–∞–Ω–∞–ª ‚Äî —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ –ë–î.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ –Ω–æ–≤—ã—Ö url'–æ–≤.
    """
    logger.info('Scanning sources for recent links (to be inserted into DB)...')
    new_added = 0
    for src in SOURCES:
        try:
            msgs = await client.get_messages(entity=src, limit=MAX_MESSAGES_POLL)
        except Exception as e:
            logger.warning(f'Polling source {src} failed: {e}')
            continue
        msgs = list(reversed(msgs))
        for m in msgs:
            if not m or not getattr(m, 'message', None):
                continue
            try:
                msg_ts = int(m.date.replace(tzinfo=pytz.utc).timestamp())
            except Exception:
                continue
            if msg_ts < cutoff:
                continue
            text = m.message
            found = URL_RE.findall(text)
            if not found:
                continue
            for raw in found:
                url_norm = normalize_url(raw)
                res = await db_fetchall('SELECT id FROM urls WHERE url=?', (url_norm,))
                if res:
                    url_id = res[0][0]
                    try:
                        await db_execute('INSERT INTO occurrences(url_id, peer, message_id, message_date) VALUES (?,?,?,?)',
                                         (url_id, str(src), m.id, msg_ts))
                    except Exception:
                        # duplicate occurrences possible ‚Äî –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º
                        pass
                else:
                    try:
                        lastrow = await db_execute('INSERT INTO urls(url, first_seen) VALUES (?,?)', (url_norm, msg_ts))
                        url_id = int(lastrow)
                        await db_execute('INSERT INTO occurrences(url_id, peer, message_id, message_date) VALUES (?,?,?,?)',
                                         (url_id, str(src), m.id, msg_ts))
                        new_added += 1
                    except Exception:
                        logger.exception('Failed to insert new url during scanning')
        await asyncio.sleep(0.25)
    logger.info(f'Scan finished. New urls inserted into DB: {new_added}')
    return new_added

async def initial_startup_sequence():
    """
    –ü–æ—Ä—è–¥–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ:
      1) —Å—Ç–∞—Ä—Ç DB worker + —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
      2) —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ä–µ–∞–∫—Ü–∏–π (–∫–∞–Ω–∞–ª -> occurrences) ‚Äî –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –î–û —É–¥–∞–ª–µ–Ω–∏—è
      3) —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ë–î (>24—á)
      4) —Å–≤–µ—Ä–∫–∞/–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–Ω–∞–ª–∞, —á—Ç–æ–±—ã –ø–æ—Ä—è–¥–æ–∫ —Å–æ–≤–ø–∞–¥–∞–ª —Å –ë–î (–µ—Å–ª–∏ —Ñ–ª–∞–≥ CLEAR_TARGET_CHANNEL_ON_START=True ‚Äî –æ—á–∏—â–∞–µ–º –∏ —Å—Ç—Ä–æ–∏–º –∑–∞–Ω–æ–≤–æ)
      5) —Å–∫–∞–Ω –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ –≤—Å—Ç–∞–≤–∫–∞ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –≤ –ë–î (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24—á)
      6) –ø–æ—Å–ª–µ –≤—Å—Ç–∞–≤–∫–∏ ‚Äî —Å–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º/–≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–∞–Ω–∞–ª (—á—Ç–æ–±—ã —É—á–µ—Å—Ç—å —Ç–æ–ª—å–∫–æ —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏) –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º sent_channel_msg_id
      7) rebuild Excel
    """
    asyncio.create_task(db_worker())
    await init_db_tables()

    now = int(time.time())
    cutoff = now - 24 * 3600

    # 1) reaction sync
    try:
        await apply_reactions_from_channel_to_sources()
    except Exception:
        logger.exception('Reaction sync failed during startup')

    # 2) cleanup old DB rows
    try:
        await cleanup_old_links()
    except Exception:
        logger.exception('Initial cleanup failed')

    # 3) if CLEAR flag True -> clear and then build using DB; else try to match DB and rebuild if mismatch
    try:
        if CLEAR_TARGET_CHANNEL_ON_START:
            # –û—á–∏—Å—Ç–∏–º –∫–∞–Ω–∞–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é (we use ensure_channel_matches_db which will clear+rebuild if mismatch)
            # But first ensure DB has current urls (maybe it's empty now) ‚Äî we will insert recent from sources below,
            # so we want to postpone the rebuild until after scanning sources. For now, if DB non-empty, we will rebuild,
            # otherwise we just clear.
            try:
                try:
                    target = int(TARGET_CHANNEL)
                except Exception:
                    target = TARGET_CHANNEL
                msgs = await client.get_messages(entity=target, limit=CHANNEL_FETCH_LIMIT)
                ids = [m.id for m in msgs if getattr(m, 'id', None) is not None]
                BATCH = 100
                for i in range(0, len(ids), BATCH):
                    batch = ids[i:i+BATCH]
                    try:
                        await client.delete_messages(entity=target, message_ids=batch)
                        await asyncio.sleep(0.2)
                    except Exception as e:
                        logger.warning(f"Failed to delete batch while clearing channel at startup: {e}")
                logger.info("Target channel cleared (startup clear).")
            except Exception:
                logger.exception("Failed to clear target channel at startup.")
        else:
            # –ï—Å–ª–∏ CLEAR=False ‚Äî –ø–æ–ø—ã—Ç–∞–µ–º—Å—è —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, –∏–Ω–∞—á–µ –ø–µ—Ä–µ—Å—Ç—Ä–æ–∏–º
            await ensure_channel_matches_db(cutoff)
    except Exception:
        logger.exception('Channel clear/sync failed during startup')

    # 4) scan sources and insert recent into DB (do not send yet)
    try:
        await scan_sources_and_insert_recent(cutoff)
    except Exception:
        logger.exception('Initial source scan failed')

    # 5) —Ç–µ–ø–µ—Ä—å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –∫–∞–Ω–∞–ª —Å DB (–ø–æ—Å–ª–µ –≤—Å—Ç–∞–≤–∫–∏ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫)
    try:
        await ensure_channel_matches_db(cutoff)
    except Exception:
        logger.exception('Final ensure channel vs DB failed')

    # 6) rebuild excel
    try:
        await rebuild_excel_from_db(EXCEL_PATH)
    except Exception:
        logger.exception('Excel rebuild failed on startup')

    logger.info('Initial startup sequence finished')

# ------------------------ Event handler (new messages) ------------------------

@client.on(events.NewMessage(chats=SOURCES))
async def handler_new_message(event):
    msg = event.message
    text = msg.message or ''
    if not text:
        return
    found = URL_RE.findall(text)
    if not found:
        return
    msg_ts = int(msg.date.replace(tzinfo=pytz.utc).timestamp())
    for raw in found:
        url = normalize_url(raw)
        try:
            res = await db_fetchall('SELECT id FROM urls WHERE url=?', (url,))
            if res:
                url_id = res[0][0]
                try:
                    await db_execute('INSERT INTO occurrences(url_id, peer, message_id, message_date) VALUES (?,?,?,?)',
                                     (url_id, str(event.chat_id), msg.id, msg_ts))
                except Exception:
                    logger.debug('Insert occurrence failed or duplicate in handler_new_message')
            else:
                lastrow = await db_execute('INSERT INTO urls(url, first_seen) VALUES (?,?)', (url, msg_ts))
                url_id = int(lastrow)
                try:
                    await db_execute('INSERT INTO occurrences(url_id, peer, message_id, message_date) VALUES (?,?,?,?)',
                                     (url_id, str(event.chat_id), msg.id, msg_ts))
                except Exception:
                    logger.debug('Insert occurrence failed after url insert')
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–≤–µ–∂–µ–µ 24—á)
                now = int(time.time())
                cutoff = now - 24 * 3600
                if msg_ts >= cutoff:
                    sent_id = None
                    try:
                        sent_id = await send_channel_message(url, msg_ts)
                    except Exception:
                        logger.exception('Failed to send to channel on new message')
                    if sent_id:
                        try:
                            await db_execute('UPDATE urls SET sent_channel_msg_id=? WHERE id=?', (sent_id, url_id))
                        except Exception:
                            logger.warning('Failed to update sent_channel_msg_id on new message')
        except Exception:
            logger.exception('Error processing new message entry')
    try:
        await rebuild_excel_from_db(EXCEL_PATH)
    except Exception:
        logger.exception('Excel rebuild failed after handling new message')

# ------------------------ MAIN / START ------------------------------

async def main():
    ensure_excel(EXCEL_PATH)
    await client.start(phone=PHONE)
    logger.info('Client started')
    await initial_startup_sequence()
    logger.info('Bot is running. Now listening for new messages...')
    try:
        await client.run_until_disconnected()
    finally:
        # –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Å—Ç–∞–Ω–æ–≤–∏–º db_worker
        await _db_queue.put(None)

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info('Interrupted by user, shutting down...')
    except Exception:
        logger.exception('Fatal error')
